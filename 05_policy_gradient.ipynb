{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 環境の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blindcliftwalk import BlindCliftwalk\n",
    "\n",
    "# ゴールまでの距離\n",
    "nb_step = 20\n",
    "#nb_step = 10\n",
    "\n",
    "# ゴールの数\n",
    "nb_goals = 3\n",
    "#nb_goals = 1\n",
    "\n",
    "env = BlindCliftwalk(nb_step, nb_goals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(seed=50)\n",
    "env.set_ground_truth()\n",
    "env.ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1077cb588>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1067abcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(env.ground_truth, 'rainbow', interpolation='nearest')\n",
    "#plt.savefig('ground_truth.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方策勾配法（REINFORCEアルゴリズム）で方策を学習する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ニューラルネット型の方策を使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruhokondo/Programming/anaconda3/envs/dl35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/ruhokondo/Programming/anaconda3/envs/dl35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "t_action = tf.placeholder(tf.float32, [None, 2], name='action')\n",
    "t_state = tf.placeholder(tf.float32, [None, 2], name='state')\n",
    "t_reward = tf.placeholder(tf.float32, [None, ], name='reward')\n",
    "\n",
    "#model = 'two_layers'\n",
    "model = 'three_layers'\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "#state_scale = 1./20.\n",
    "state_scale = 1.\n",
    "\n",
    "if model=='two_layers':\n",
    "    nb_nodes = 20\n",
    "    \n",
    "    W1 = tf.Variable(tf.random_normal([2, nb_nodes], stddev=np.sqrt(2. / float(2 + nb_nodes))), name='W1')\n",
    "    W2 = tf.Variable(tf.random_normal([nb_nodes, 2], stddev=np.sqrt(2. / float(nb_nodes + 2))), name='W2')\n",
    "    b1 = tf.Variable(tf.random_normal([nb_nodes]), name='b1')\n",
    "    b2 = tf.Variable(tf.random_normal([2]), name='b2')\n",
    "    \n",
    "    h1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(t_state, W1),b1))\n",
    "    t_policy = tf.nn.softmax(tf.nn.bias_add(tf.matmul(h1, W2),b2))\n",
    "#    h1 = tf.nn.relu(tf.contrib.layers.batch_norm(tf.nn.bias_add(tf.matmul(t_state, W1),b1)))\n",
    "#    t_policy = tf.nn.softmax(tf.contrib.layers.batch_norm(tf.nn.bias_add(tf.matmul(h1, W2),b2)))\n",
    "    \n",
    "elif model=='three_layers':\n",
    "    nb_nodes1 = 20\n",
    "    nb_nodes2 = 20\n",
    "    \n",
    "    W1 = tf.Variable(tf.random_normal([2, nb_nodes1], stddev=np.sqrt(2. / float(2 + nb_nodes1))), name='W1')\n",
    "    W2 = tf.Variable(tf.random_normal([nb_nodes1, nb_nodes2], stddev=np.sqrt(2. / float(nb_nodes1 + nb_nodes2))), name='W2')\n",
    "    W3 = tf.Variable(tf.random_normal([nb_nodes2, 2], stddev=np.sqrt(2. / float(nb_nodes2 + 2))), name='W3')\n",
    "    b1 = tf.Variable(tf.random_normal([nb_nodes1]), name='b1')\n",
    "    b2 = tf.Variable(tf.random_normal([nb_nodes2]), name='b2')\n",
    "    b3 = tf.Variable(tf.random_normal([2]), name='b3')\n",
    "    \n",
    "    h1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(t_state, W1),b1))\n",
    "    h2 = tf.nn.relu(tf.nn.bias_add(tf.matmul(h1, W2),b2))\n",
    "    t_policy = tf.nn.softmax(tf.nn.bias_add(tf.matmul(h2, W3),b3))\n",
    "\n",
    "'''\n",
    "p(a=a*|s) = <p(a|s), a*>\n",
    "ただし、a*はone-hotベクトル\n",
    "'''\n",
    "t_score = tf.reduce_sum(t_policy * t_action, -1)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "\n",
    "'''\n",
    "方策勾配法では損失関数ではなく方策の良さを目的関数にしているので、\n",
    "パラメータの更新方向は勾配方向と同じ（損失関数を使った場合と逆向き）\n",
    "'''\n",
    "grads_and_vars = optimizer.compute_gradients(-t_reward * t_score)\n",
    "\n",
    "train_step = optimizer.apply_gradients(grads_and_vars)\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  エポック数。全エピソード数は nb_epoch * nb_mc になる。\n",
    "nb_epoch = 1000\n",
    "\n",
    "# MC計算に使うepisode数。パラメータ更新回数は　nb_episode　/　nb_mc　になる\n",
    "nb_mc = 100\n",
    "\n",
    "# ゴール到達後のstate valueの保存間隔\n",
    "save_interval = 1\n",
    "\n",
    "# ε-greedyのハイパーパラメータ\n",
    "epsilon = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_sampling(prob):\n",
    "    rand = np.random.random(len(prob))\n",
    "    g = -np.log(-np.log(rand))\n",
    "    return np.argmax(g+np.log(prob))\n",
    "\n",
    "def epsilon_greedy(sess, t_policy, state, env, epsilon):\n",
    "    if np.random.random(1) > epsilon:\n",
    "        action_prob = t_policy.eval({t_state:np.expand_dims(np.array(state) * state_scale,0)}, sess)[0]# + np.finfo(np.float32).eps\n",
    "    else:\n",
    "        action_prob = np.random.random(2)\n",
    "    action_prob = action_prob / action_prob.sum()\n",
    "    action = np.eye(2)[gumbel_sampling(action_prob)].astype(int)\n",
    "    return action_prob, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(sess, env, nb_try=100):\n",
    "    trajectory = np.zeros_like(env.ground_truth).astype(np.float32)\n",
    "\n",
    "    for _ in range(nb_try):\n",
    "        current_state = [0,0]\n",
    "        status = 1\n",
    "        while status>=0:\n",
    "\n",
    "                # policyにしたがって行動を取る（まだ遷移させない）。εはゼロにしておく。\n",
    "                action_prob, action = epsilon_greedy(sess, t_policy, current_state, env, 0.)\n",
    "\n",
    "                # 行動に対する報酬と遷移後の状態の計算\n",
    "                next_state, reward, status = env.do_action(current_state, action)\n",
    "\n",
    "                # トラジェクトリの記録\n",
    "                trajectory[current_state[0],current_state[1]] += 1.\n",
    "\n",
    "                # 状態の遷移（マス目の移動）\n",
    "                current_state = next_state\n",
    "    return trajectory\n",
    "\n",
    "def get_probability_distribution(env, sess):\n",
    "    learnt_policy = np.expand_dims(np.zeros_like(env.ground_truth).astype(np.float32), -1)\n",
    "    learnt_policy = np.concatenate([learnt_policy,learnt_policy,learnt_policy],-1)\n",
    "    for s0 in range(env.ground_truth.shape[0]):\n",
    "        for s1 in range(env.ground_truth.shape[1]):\n",
    "            state = np.array([s0,s1]).astype(np.float32)\n",
    "            policy = t_policy.eval({t_state: np.expand_dims(state,0)}, sess)[0]\n",
    "            \n",
    "            # Rチャンネルにdownの確率、Bチャンネルにrightの確率\n",
    "            learnt_policy[s0,s1,0] = policy[0]\n",
    "            learnt_policy[s0,s1,2] = policy[1]\n",
    "    return learnt_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [10:51<00:00,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "isSuccessed = False\n",
    "\n",
    "temp = []\n",
    "count = -1\n",
    "success_history = []\n",
    "for epoch in tqdm(range(int(nb_epoch))):\n",
    "    \n",
    "    # ε-greedyのεを減衰させる\n",
    "    epsilon = epsilon * 0.999\n",
    "    \n",
    "    episode_states = []\n",
    "    episode_actions = []\n",
    "    episode_rewards = []\n",
    "    success_count = 0\n",
    "    if not epoch%save_interval:\n",
    "        count+=1\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(env.ground_truth, 'rainbow', interpolation='nearest')\n",
    "        plt.title('ground truth')\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(get_probability_distribution(env, sess), 'bwr', interpolation='nearest')\n",
    "        plt.title('probability')\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(np.log(get_trajectory(sess, env)+np.finfo(np.float32).eps), 'hot')\n",
    "        plt.title('trajectory')\n",
    "        plt.savefig('./png/REINFORCE_'+str(count).zfill(4)+'.png')\n",
    "#        plt.show()\n",
    "        plt.close()\n",
    "#        print()\n",
    "    for mc_step in range(nb_mc):\n",
    "        # 状態の初期化（スタート地点に戻る）\n",
    "        current_state  = np.zeros(2, dtype=int)\n",
    "    \n",
    "        status = 0\n",
    "        while status >= 0:\n",
    "            # policyにしたがって行動を取る（まだ遷移させない）\n",
    "            action_prob, action = epsilon_greedy(sess, t_policy, current_state, env, epsilon)\n",
    "\n",
    "            # 行動に対する報酬と遷移後の状態の計算\n",
    "            next_state, reward, status = env.do_action(current_state, action)\n",
    "#            if status==0:\n",
    "#                reward=1#test\n",
    "#            if status==1:\n",
    "#                reward=10#test\n",
    "            if status==-1:\n",
    "                reward=-1#test\n",
    "#                reward=-0.1#test\n",
    "                \n",
    "            if not epoch%save_interval and mc_step==0:\n",
    "                pass\n",
    "#                print('s=',current_state,' p=',action_prob,' a=',action,' status=',status)\n",
    "            \n",
    "            # エピソード中の遷移前状態、行動、報酬を保存する\n",
    "            episode_states.append(copy.deepcopy(current_state))\n",
    "            episode_actions.append(copy.deepcopy(action))\n",
    "            episode_rewards.append(copy.deepcopy(reward))\n",
    "\n",
    "            # 状態の遷移（マス目の移動）\n",
    "            current_state = next_state\n",
    "            \n",
    "            if status==1:\n",
    "                success_count += 1\n",
    "                isSuccessed = True\n",
    "\n",
    "    # 方策勾配を計算する\n",
    "    episode_states = np.array(episode_states)\n",
    "    episode_actions = np.array(episode_actions)\n",
    "    episode_rewards = np.array(episode_rewards)\n",
    "    success_history.append(success_count)\n",
    "    sess.run(\n",
    "        [train_step], \n",
    "        feed_dict={\n",
    "            t_action: episode_actions,\n",
    "#            t_state: episode_states,\n",
    "            t_state: episode_states * state_scale,\n",
    "            t_reward: episode_rewards\n",
    "#            t_reward: episode_rewards - episode_rewards.mean() # 平均報酬をベースラインとする\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(success_history)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('success count / epoch')\n",
    "plt.show()\n",
    "\n",
    "if False:\n",
    "    for i in range(len(success_history)):\n",
    "        plt.plot(success_history[:i])\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('success count / epoch')\n",
    "        plt.xlim(0,1000)\n",
    "        plt.ylim(0,9)\n",
    "        plt.savefig('./png/REINCORCE_learning_curve_'+str(i).zfill(4)+'.png', bbox_inches='tight', transparent=True)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl35]",
   "language": "python",
   "name": "conda-env-dl35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
